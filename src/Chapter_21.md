# 第21章 多任务微调
## 简介
- 多任务微调是单任务微调的扩展，其中训练数据集由多个任务的示例输入和输出组成。
- 比如数据集包含指示模型执行各种任务的示例，包括摘要、评论评分、代码翻译和实体识别，在这个混合数据集上训练模型，以便它可以同时提高模型在所有任务上的性能，从而避免灾难性遗忘的问题。
- 经过多个训练周期后，通过计算示例的损失来更新模型的权重，从而得到一个经过指令微调的模型，该模型学会同时擅长许多不同的任务。
- 多任务微调的一个缺点是它需要大量数据。您可能需要在训练集中包含多达50,000到100,000个示例。
- 但收集这些数据是非常值得的。生成的模型通常非常强大，适用于需要在许多任务上表现良好的情况。

## 示例
指令模型的差异取决于微调过程中使用的数据集和任务。
### FLAN简介
- FLAN代表“Fine-tuned Language Net”，是用于微调不同模型的一组特定指令。
- 由于FLAN微调是训练过程的最后一步，原始论文的作者将其称为预训练主菜后的甜点。
- FLAN-T5是T5基础模型的FLAN指令版本，而FLAN-PALM是PALM基础模型的FLAN指令版本。
### FLAN-T5微调
- FLAN-T5是一个很好的通用指令模型。它在146个任务类别中的473个数据集上进行了微调。
- 用于FLAN-T5摘要任务的一个提示数据集示例是SAMSum。它是MUFFIN任务和数据集集合的一部分，用于训练语言模型以总结对话。
- SAMSum是一个包含16,000个类似即时通讯对话及其摘要的数据集，这些对话和摘要由语言学家精心制作。
- 与SAMSum对话摘要数据集一起使用的提示模板，要求模型总结对话，SAMSum数据集中的对话都会插入到模板中“对话”字段出现的位置。
- 如果实际场景与数据集的语言结构重叠不多，需要使用更接近的对话数据集对FLAN-T5模型进行额外的微调。
