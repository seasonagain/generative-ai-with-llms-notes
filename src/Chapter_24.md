# 第24章 Benchmarks基准测试
## 评估的重要性
- LLMs 复杂，简单评估指标（如 ROUGE 和 BLUR）提供的信息有限。
- 使用专门设计的基准和数据集可以更全面地衡量和比较 LLMs。

## 数据集选择
- **选择合适的数据集**：准确评估模型表现，理解其能力。
- **隔离特定技能**：如推理、常识知识。
- **关注潜在风险**：如虚假信息、版权侵权。
- **避免训练数据泄露**：使用未见过的数据进行评估，结果更准确。

## 常用基准
### 1. GLUE（通用语言理解评估）
- **推出时间**：2018 年。
- **内容**：包含情感分析、问答等自然语言任务。
- **目标**：鼓励开发能跨任务泛化的模型。

### 2. SuperGLUE
- **推出时间**：2019 年。
- **改进**：解决 GLUE 的局限性，包含更复杂的任务。
- **任务示例**：多句推理、阅读理解。
- **特点**：提供排行榜，便于模型比较。

### 3. MMLU（大规模多任务语言理解）
- **目标**：测试现代 LLMs 的世界知识和问题解决能力。
- **任务领域**：基础数学、美国历史、计算机科学、法律等。

### 4. BIG-bench
- **任务数量**：204 个。
- **领域**：语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、软件开发等。
- **规模**：提供三种规模，以控制推理成本。

### 5. HELM（语言模型整体评估）
- **目标**：提高模型透明度，提供任务表现指导。
- **特点**：
  - 多指标评估：16 个核心场景，7 个指标。
  - 超越准确性：包括公平性、偏见、毒性等指标。
  - 持续演进：不断添加新场景、指标和模型。
- **用途**：浏览评估结果，选择适合项目的模型。

## 总结
- 选择合适的基准和数据集是评估 LLMs 的关键。
- 随着模型能力的提升，基准也在不断演进，以更全面地衡量模型表现。
