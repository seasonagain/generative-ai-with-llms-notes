# 第20章 单一任务微调
## 介绍
- 大型语言模型（LLMs）能执行多种语言任务，但应用程序可能只需单一任务。此时可微调预训练模型，提升该单一任务的性能。
- 例如，用该任务的示例数据集做摘要生成，只需500到1,000个示例就能取得良好效果，无需模型预训练时的数十亿条文本。

## 单一任务微调潜在缺点
- 单一任务微调可能引发**灾难性遗忘**现象。
- 完整的微调过程会修改原始LLM的权重，虽能提升单一任务性能，但会降低模型在其他任务上的表现。
- 例如，微调可提高模型在评论情感分析上的能力，但可能使模型忘记如何执行其他任务，如命名实体识别。微调前，模型能正确识别句子中猫的名字“Charlie”；微调后，不仅无法完成此任务，还会出现与新任务相关的混淆行为。

## 如何避免灾难性遗忘
### 选项1：多任务微调
- 确定灾难性遗忘是否真的影响您的用例。如果您只需要在微调的单一任务上获得可靠的性能，那么模型无法泛化到其他任务可能并不是问题。
- 如果确实希望或需要模型保持其多任务泛化能力，您可以同时对多个任务进行微调。
- 良好的多任务微调可能需要50,000到100,000个示例，涵盖多个任务，因此需要更多的数据和计算资源来进行训练。

### 选项2：参数高效微调
- **参数高效微调**（Parameter Efficient Fine-Tuning，简称PEFT），旨在保留原始LLM的权重，并仅训练少量特定任务的适配层和参数。
- 由于大部分预训练权重保持不变，PEFT对灾难性遗忘表现出更强的鲁棒性。